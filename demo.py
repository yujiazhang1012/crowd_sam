import os
import cv2
import torch
import numpy as np
from PIL import Image
import torchvision.transforms as T
import sys

# ====== ä¿®æ”¹è¿™é‡Œï¼šè®¾ç½®ä½ çš„é¡¹ç›®è·¯å¾„ ======
sys.path.append("/home/ccnu-train/zyj/crowd_sam/crowd_sam")  # ä½ çš„ CrowdSAM é¡¹ç›®è·¯å¾„

from crowdsam.model import CrowdSAM
import crowdsam.utils as utils


def visualize_results(image, mask_data, action_classes):
    """
    åœ¨å›¾åƒä¸Šç»˜åˆ¶æ£€æµ‹æ¡†å’ŒåŠ¨ä½œæ ‡ç­¾
    """
    # å®šä¹‰é¢œè‰²ï¼ˆæ¯ä¸ªåŠ¨ä½œä¸€ä¸ªé¢œè‰²ï¼‰
    colors = [
        (0, 255, 0),    # write - ç»¿è‰²
        (255, 0, 0),    # read - è“è‰²
        (0, 0, 255),    # lookup - çº¢è‰²
        (255, 255, 0),  # turn_head - é’è‰²
        (255, 0, 255),  # raise_hand - å“çº¢
        (0, 255, 255),  # stand - é»„è‰²
        (128, 128, 128) # discuss - ç°è‰²
    ]
    
    # è½¬æ¢ä¸º BGR æ ¼å¼ï¼ˆOpenCV ä½¿ç”¨ BGRï¼‰
    if image.shape[2] == 3:
        image_bgr = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)
    else:
        image_bgr = image.copy()

    # å®‰å…¨æ£€æŸ¥ï¼šä½¿ç”¨hasattr æˆ–ç›´æŽ¥è®¿é—®
    has_actions = hasattr(mask_data, '_stats') and 'actions' in mask_data._stats
    
    # ç»˜åˆ¶æ¯ä¸ªæ£€æµ‹ç»“æžœ
    # if 'actions' in mask_data and len(mask_data['actions']) > 0:
    if has_actions and len(mask_data['boxes']) > 0:
        boxes = mask_data['boxes'].numpy() if hasattr(mask_data['boxes'], 'numpy') else mask_data['boxes']
        actions = mask_data['actions']
        
        for i, (box, action) in enumerate(zip(boxes, actions)):
            x1, y1, x2, y2 = map(int, box)
            action_id = action['action_id']
            action_name = action['action_name']
            confidence = action['confidence']
            
            # èŽ·å–é¢œè‰²
            color = colors[action_id % len(colors)]
            
            # ç»˜åˆ¶è¾¹ç•Œæ¡†
            cv2.rectangle(image_bgr, (x1, y1), (x2, y2), color, 2)
            
            # ç»˜åˆ¶æ ‡ç­¾
            label = f"{action_name}: {confidence:.2f}"
            cv2.putText(image_bgr, label, (x1, y1 - 10), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)
    
    return image_bgr


def main():
    CONFIG_PATH = "configs/crowdhuman.yaml"        # é…ç½®æ–‡ä»¶è·¯å¾„
    IMAGE_PATH = "dataset/crowdhuman/Images/036.jpg"       # è¾“å…¥å›¾åƒè·¯å¾„
    OUTPUT_PATH = "demo_output_5.jpg"                # è¾“å‡ºå›¾åƒè·¯å¾„
    DEVICE = "cuda"                                # è®¾å¤‡ (cuda æˆ– cpu)
    ACTION_HEAD_PATH = "weights/action_head_best.pth"   # åŠ¨ä½œè¯†åˆ«å¤´æƒé‡è·¯å¾„

    # åŠ è½½é…ç½®
    config = utils.load_config(CONFIG_PATH)
    config['environ']['device'] = DEVICE

    # åˆå§‹åŒ–æ¨¡åž‹
    print("Loading CrowdSAM model...")
    model = CrowdSAM(config, logger=None)
    model.eval()

    # æ·»åŠ è°ƒè¯•
    # print(f"ðŸŽ¯ Model action_classes: {model.action_classes}")
    # print(f"ðŸŽ¯ Number of action classes: {model.num_action_classes}")
    # print(f"ðŸŽ¯ Action head architecture: {model.action_head}")


    
    # åŠ è½½åŠ¨ä½œè¯†åˆ«å¤´æƒé‡ï¼ˆå¦‚æžœå­˜åœ¨ï¼‰
    if os.path.exists(ACTION_HEAD_PATH):
        print(f"âœ… Found weights at: {ACTION_HEAD_PATH}")
        model.action_head.load_state_dict(
            torch.load(ACTION_HEAD_PATH, map_location=model.device)
        )
        print(f"Loaded action head weights from {ACTION_HEAD_PATH}")
    else:
        print(f"Warning: Action head weights not found at {ACTION_HEAD_PATH}!")

    # åŠ è½½å›¾åƒ
    print(f"Loading image from {IMAGE_PATH}")
    image = cv2.imread(IMAGE_PATH)
    if image is None:
        raise ValueError(f"Could not load image from {IMAGE_PATH}")

    print(f"Image shape: {image.shape}")  # æ·»åŠ è°ƒè¯•
    print(f"Image dtype: {image.dtype}")  # æ·»åŠ è°ƒè¯•
    
    
    # è½¬æ¢ä¸º RGB
    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    print(f"RGB image shape: {image_rgb.shape}")  # æ·»åŠ è°ƒè¯•

    # æŽ¨ç†
    print("Running inference...")
    with torch.no_grad():
        mask_data = model.generate(image_rgb)
   
    print(f"mask_data keys: {list(mask_data._stats.keys())}")
    if 'masks' in mask_data._stats:
        print(f"Number of masks: {len(mask_data['masks'])}")
    else:
        print("No masks generated! Check your input image and model weights.")

    # å¯è§†åŒ–ç»“æžœ
    print("Visualizing results...")
    result_image = visualize_results(image_rgb, mask_data, model.action_classes)

    # ä¿å­˜ç»“æžœ
    cv2.imwrite(OUTPUT_PATH, result_image)
    print(f"Results saved to {OUTPUT_PATH}")

    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    if 'actions' in mask_data._stats and len(mask_data['actions']) > 0:
        actions = mask_data._stats['actions']  # ç›´æŽ¥è®¿é—® _stats
        num_detections = len(actions)
        print(f"\nðŸ“Š Detected {num_detections} persons")
        action_count = {}
        for i, action in enumerate(actions):
            action_name = action['action_name']
            if action_name not in action_count:
                action_count[action_name] = 0
            action_count[action_name] += 1
            print(f"  Person {i+1}: {action_name} (conf: {action['confidence']:.2f})")
        
        print("\nðŸ“ˆ Action distribution:")
        for action_name, count in action_count.items():
            print(f"  {action_name}: {count}")
    else:
        print("No persons detected!")


if __name__ == '__main__':
    main()